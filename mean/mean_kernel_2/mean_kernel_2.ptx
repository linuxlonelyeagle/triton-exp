//
// Generated by LLVM NVPTX Back-End
//

.version 7.5
.target sm_80
.address_size 64

	// .globl	mean_kernel_2_0d1d23d
.extern .shared .align 1 .b8 global_smem[];

.visible .entry mean_kernel_2_0d1d23d(
	.param .u64 mean_kernel_2_0d1d23d_param_0,
	.param .u64 mean_kernel_2_0d1d23d_param_1,
	.param .u32 mean_kernel_2_0d1d23d_param_2,
	.param .u32 mean_kernel_2_0d1d23d_param_3
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<31>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<46>;

	ld.param.u64 	%rd8, [mean_kernel_2_0d1d23d_param_0];
	ld.param.u64 	%rd7, [mean_kernel_2_0d1d23d_param_1];
	mov.u32 	%r26, %tid.x;
	and.b32  	%r27, %r26, 31;
	ld.param.u32 	%r28, [mean_kernel_2_0d1d23d_param_2];
	ld.param.u32 	%r29, [mean_kernel_2_0d1d23d_param_3];
	mul.wide.u32 	%rd9, %r27, 8;
	add.s64 	%rd2, %rd8, %rd9;
	setp.lt.s32 	%p1, %r27, %r29;
	mov.u64 	%rd3, 0;
	@%p1 ld.global.b64 { %rd1 }, [ %rd2 + 0 ];
	@!%p1 mov.u64 %rd1, %rd3;
	shr.u64 	%rd10, %rd1, 32;
	cvt.u32.u64 	%r4, %rd10;
	cvt.u32.u64 	%r2, %rd1;
	setp.eq.s32 	%p3, %r27, 0;
	shfl.sync.bfly.b32 %r1, %r2, 0x10, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r3, %r4, 0x10, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd11, %r1;
	cvt.u64.u32 	%rd12, %r3;
	shl.b64 	%rd13, %rd12, 32;
	or.b64  	%rd14, %rd11, %rd13;
	add.s64 	%rd15, %rd1, %rd14;
	shr.u64 	%rd16, %rd15, 32;
	cvt.u32.u64 	%r8, %rd16;
	cvt.u32.u64 	%r6, %rd15;
	shfl.sync.bfly.b32 %r5, %r6, 0x8, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r7, %r8, 0x8, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd17, %r5;
	cvt.u64.u32 	%rd18, %r7;
	shl.b64 	%rd19, %rd18, 32;
	or.b64  	%rd20, %rd17, %rd19;
	add.s64 	%rd21, %rd15, %rd20;
	shr.u64 	%rd22, %rd21, 32;
	cvt.u32.u64 	%r12, %rd22;
	cvt.u32.u64 	%r10, %rd21;
	shfl.sync.bfly.b32 %r9, %r10, 0x4, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r11, %r12, 0x4, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd23, %r9;
	cvt.u64.u32 	%rd24, %r11;
	shl.b64 	%rd25, %rd24, 32;
	or.b64  	%rd26, %rd23, %rd25;
	add.s64 	%rd27, %rd21, %rd26;
	shr.u64 	%rd28, %rd27, 32;
	cvt.u32.u64 	%r16, %rd28;
	cvt.u32.u64 	%r14, %rd27;
	shfl.sync.bfly.b32 %r13, %r14, 0x2, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r15, %r16, 0x2, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd29, %r13;
	cvt.u64.u32 	%rd30, %r15;
	shl.b64 	%rd31, %rd30, 32;
	or.b64  	%rd32, %rd29, %rd31;
	add.s64 	%rd33, %rd27, %rd32;
	shr.u64 	%rd34, %rd33, 32;
	cvt.u32.u64 	%r20, %rd34;
	cvt.u32.u64 	%r18, %rd33;
	shfl.sync.bfly.b32 %r17, %r18, 0x1, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r19, %r20, 0x1, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd35, %r17;
	cvt.u64.u32 	%rd36, %r19;
	shl.b64 	%rd37, %rd36, 32;
	or.b64  	%rd38, %rd35, %rd37;
	add.s64 	%rd4, %rd33, %rd38;
	mov.u32 	%r21, global_smem;
	@%p3 st.shared.b64 [ %r21 + 0 ], %rd4;
	bar.sync 	0;
	shl.b32 	%r30, %r26, 3;
	add.s32 	%r22, %r21, %r30;
	ld.shared.u32 	%rd39, [%r22];
	ld.shared.u32 	%rd40, [%r22+4];
	shl.b64 	%rd41, %rd40, 32;
	or.b64  	%rd5, %rd41, %rd39;
	setp.lt.s32 	%p4, %r26, 1;
	@%p4 st.shared.b64 [ %r22 + 0 ], %rd5;
	bar.sync 	0;
	ld.shared.u32 	%rd42, [global_smem+4];
	shl.b64 	%rd43, %rd42, 32;
	ld.shared.u32 	%rd44, [global_smem];
	or.b64  	%rd45, %rd43, %rd44;
	cvt.rn.f32.s64 	%f1, %rd45;
	cvt.rn.f32.s32 	%f2, %r28;
	mov.b32 	%r24, %f1;
	mov.b32 	%r25, %f2;
	div.full.f32 %r23, %r24, %r25;
	mov.b32 	%f3, %r23;
	cvt.rzi.s64.f32 	%rd6, %f3;
	mov.pred 	%p5, -1;
	@%p5 st.global.b64 [ %rd7 + 0 ], { %rd6 };
	ret;

}
