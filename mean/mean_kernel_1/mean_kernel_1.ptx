//
// Generated by LLVM NVPTX Back-End
//

.version 7.5
.target sm_80
.address_size 64

	// .globl	mean_kernel_1_0d1d2
.extern .shared .align 1 .b8 global_smem[];

.visible .entry mean_kernel_1_0d1d2(
	.param .u64 mean_kernel_1_0d1d2_param_0,
	.param .u64 mean_kernel_1_0d1d2_param_1,
	.param .u32 mean_kernel_1_0d1d2_param_2
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<9>;
	.reg .b32 	%r<39>;
	.reg .b64 	%rd<52>;

	ld.param.u64 	%rd8, [mean_kernel_1_0d1d2_param_0];
	ld.param.u64 	%rd9, [mean_kernel_1_0d1d2_param_1];
	mov.u32 	%r27, %tid.x;
	and.b32  	%r28, %r27, 31;
	ld.param.u32 	%r29, [mean_kernel_1_0d1d2_param_2];
	shr.u32 	%r30, %r27, 2;
	and.b32  	%r31, %r30, 1073741816;
	and.b32  	%r32, %r27, 63;
	mov.u32 	%r33, %ctaid.x;
	shl.b32 	%r34, %r33, 6;
	or.b32  	%r35, %r34, %r32;
	mul.wide.s32 	%rd10, %r35, 8;
	add.s64 	%rd2, %rd8, %rd10;
	setp.lt.s32 	%p1, %r35, %r29;
	mov.u64 	%rd3, 0;
	@%p1 ld.global.b64 { %rd1 }, [ %rd2 + 0 ];
	@!%p1 mov.u64 %rd1, %rd3;
	shr.u64 	%rd11, %rd1, 32;
	cvt.u32.u64 	%r4, %rd11;
	cvt.u32.u64 	%r2, %rd1;
	setp.eq.s32 	%p3, %r28, 0;
	shfl.sync.bfly.b32 %r1, %r2, 0x10, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r3, %r4, 0x10, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd12, %r1;
	cvt.u64.u32 	%rd13, %r3;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd15, %rd12, %rd14;
	add.s64 	%rd16, %rd1, %rd15;
	shr.u64 	%rd17, %rd16, 32;
	cvt.u32.u64 	%r8, %rd17;
	cvt.u32.u64 	%r6, %rd16;
	shfl.sync.bfly.b32 %r5, %r6, 0x8, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r7, %r8, 0x8, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd18, %r5;
	cvt.u64.u32 	%rd19, %r7;
	shl.b64 	%rd20, %rd19, 32;
	or.b64  	%rd21, %rd18, %rd20;
	add.s64 	%rd22, %rd16, %rd21;
	shr.u64 	%rd23, %rd22, 32;
	cvt.u32.u64 	%r12, %rd23;
	cvt.u32.u64 	%r10, %rd22;
	shfl.sync.bfly.b32 %r9, %r10, 0x4, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r11, %r12, 0x4, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd24, %r9;
	cvt.u64.u32 	%rd25, %r11;
	shl.b64 	%rd26, %rd25, 32;
	or.b64  	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd22, %rd27;
	shr.u64 	%rd29, %rd28, 32;
	cvt.u32.u64 	%r16, %rd29;
	cvt.u32.u64 	%r14, %rd28;
	shfl.sync.bfly.b32 %r13, %r14, 0x2, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r15, %r16, 0x2, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd30, %r13;
	cvt.u64.u32 	%rd31, %r15;
	shl.b64 	%rd32, %rd31, 32;
	or.b64  	%rd33, %rd30, %rd32;
	add.s64 	%rd34, %rd28, %rd33;
	shr.u64 	%rd35, %rd34, 32;
	cvt.u32.u64 	%r20, %rd35;
	cvt.u32.u64 	%r18, %rd34;
	shfl.sync.bfly.b32 %r17, %r18, 0x1, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r19, %r20, 0x1, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd36, %r17;
	cvt.u64.u32 	%rd37, %r19;
	shl.b64 	%rd38, %rd37, 32;
	or.b64  	%rd39, %rd36, %rd38;
	add.s64 	%rd4, %rd34, %rd39;
	mov.u32 	%r36, global_smem;
	add.s32 	%r21, %r36, %r31;
	@%p3 st.shared.b64 [ %r21 + 0 ], %rd4;
	bar.sync 	0;
	shl.b32 	%r37, %r27, 3;
	add.s32 	%r26, %r36, %r37;
	ld.shared.u32 	%rd40, [%r26];
	ld.shared.u32 	%rd41, [%r26+4];
	shl.b64 	%rd42, %rd41, 32;
	or.b64  	%rd43, %rd42, %rd40;
	cvt.u32.u64 	%r25, %rd41;
	cvt.u32.u64 	%r23, %rd40;
	shfl.sync.bfly.b32 %r22, %r23, 0x1, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r24, %r25, 0x1, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd44, %r22;
	cvt.u64.u32 	%rd45, %r24;
	shl.b64 	%rd46, %rd45, 32;
	or.b64  	%rd47, %rd44, %rd46;
	add.s64 	%rd5, %rd43, %rd47;
	setp.lt.s32 	%p6, %r27, 2;
	and.b32  	%r38, %r27, 1;
	setp.eq.b32 	%p7, %r38, 1;
	not.pred 	%p8, %p7;
	and.pred  	%p4, %p6, %p8;
	@%p4 st.shared.b64 [ %r26 + 0 ], %rd5;
	bar.sync 	0;
	ld.shared.u32 	%rd48, [global_smem+4];
	shl.b64 	%rd49, %rd48, 32;
	ld.shared.u32 	%rd50, [global_smem];
	or.b64  	%rd6, %rd49, %rd50;
	mul.wide.s32 	%rd51, %r33, 8;
	add.s64 	%rd7, %rd9, %rd51;
	mov.pred 	%p5, -1;
	@%p5 st.global.b64 [ %rd7 + 0 ], { %rd6 };
	ret;

}
