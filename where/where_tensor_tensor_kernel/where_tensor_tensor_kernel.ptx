//
// Generated by LLVM NVPTX Back-End
//

.version 7.5
.target sm_80
.address_size 64

	// .globl	where_tensor_tensor_kernel_0d1d2d3d4d

.visible .entry where_tensor_tensor_kernel_0d1d2d3d4d(
	.param .u64 where_tensor_tensor_kernel_0d1d2d3d4d_param_0,
	.param .u64 where_tensor_tensor_kernel_0d1d2d3d4d_param_1,
	.param .u64 where_tensor_tensor_kernel_0d1d2d3d4d_param_2,
	.param .u64 where_tensor_tensor_kernel_0d1d2d3d4d_param_3,
	.param .u32 where_tensor_tensor_kernel_0d1d2d3d4d_param_4
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<6>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<10>;
	.reg .f32 	%f<4>;
	.reg .b64 	%rd<11>;

	ld.param.u64 	%rd5, [where_tensor_tensor_kernel_0d1d2d3d4d_param_0];
	ld.param.u64 	%rd6, [where_tensor_tensor_kernel_0d1d2d3d4d_param_1];
	mov.u32 	%r4, %tid.x;
	and.b32  	%r5, %r4, 3;
	ld.param.u64 	%rd7, [where_tensor_tensor_kernel_0d1d2d3d4d_param_2];
	ld.param.u64 	%rd8, [where_tensor_tensor_kernel_0d1d2d3d4d_param_3];
	mov.u32 	%r6, %ctaid.x;
	shl.b32 	%r7, %r6, 2;
	ld.param.u32 	%r8, [where_tensor_tensor_kernel_0d1d2d3d4d_param_4];
	or.b32  	%r9, %r7, %r5;
	setp.lt.s32 	%p1, %r9, %r8;
	cvt.s64.s32 	%rd9, %r9;
	add.s64 	%rd1, %rd5, %rd9;
	@%p1 ld.global.b8 { %rs1 }, [ %rd1 + 0 ];
	and.b16  	%rs2, %rs1, 255;
	mul.wide.s32 	%rd10, %r9, 4;
	add.s64 	%rd2, %rd6, %rd10;
	@%p1 ld.global.b32 { %r1 }, [ %rd2 + 0 ];
	mov.b32 	%f1, %r1;
	add.s64 	%rd3, %rd7, %rd10;
	@%p1 ld.global.b32 { %r2 }, [ %rd3 + 0 ];
	mov.b32 	%f2, %r2;
	setp.eq.s16 	%p5, %rs2, 0;
	selp.f32 	%f3, %f2, %f1, %p5;
	add.s64 	%rd4, %rd8, %rd10;
	mov.b32 	%r3, %f3;
	@%p1 st.global.b32 [ %rd4 + 0 ], { %r3 };
	ret;

}
