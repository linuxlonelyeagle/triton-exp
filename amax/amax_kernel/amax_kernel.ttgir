#blocked0 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [32, 1], warpsPerCTA = [1, 4], order = [0, 1]}>
#blocked1 = #triton_gpu.blocked<{sizePerThread = [1, 1], threadsPerWarp = [4, 8], warpsPerCTA = [4, 1], order = [1, 0]}>
module attributes {"triton_gpu.num-warps" = 4 : i32} {
  func public @amax_kernel_0d1d23(%arg0: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg1: !tt.ptr<f32> {tt.divisibility = 16 : i32}, %arg2: i32, %arg3: i32) {
    %c32_i32 = arith.constant 32 : i32
    %cst = arith.constant dense<0xFF800000> : tensor<32x8xf32, #blocked1>
    %c0 = arith.constant 0 : index
    %c8 = arith.constant 8 : index
    %0 = tt.get_program_id {axis = 0 : i32} : i32
    %1 = arith.muli %0, %c32_i32 : i32
    %2 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>
    %3 = tt.make_range {end = 32 : i32, start = 0 : i32} : tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>
    %4 = tt.expand_dims %2 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>) -> tensor<32x1xi32, #blocked0>
    %5 = tt.expand_dims %3 {axis = 1 : i32} : (tensor<32xi32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<32x1xi32, #blocked1>
    %6 = tt.splat %1 : (i32) -> tensor<32x1xi32, #blocked0>
    %7 = tt.splat %1 : (i32) -> tensor<32x1xi32, #blocked1>
    %8 = arith.addi %6, %4 : tensor<32x1xi32, #blocked0>
    %9 = arith.addi %7, %5 : tensor<32x1xi32, #blocked1>
    %10 = tt.splat %arg3 : (i32) -> tensor<32x1xi32, #blocked1>
    %11 = arith.muli %9, %10 : tensor<32x1xi32, #blocked1>
    %12 = tt.splat %arg0 : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked1>
    %13 = tt.addptr %12, %11 : tensor<32x1x!tt.ptr<f32>, #blocked1>, tensor<32x1xi32, #blocked1>
    %14 = tt.splat %arg1 : (!tt.ptr<f32>) -> tensor<32x1x!tt.ptr<f32>, #blocked0>
    %15 = tt.addptr %14, %8 : tensor<32x1x!tt.ptr<f32>, #blocked0>, tensor<32x1xi32, #blocked0>
    %16 = tt.splat %arg2 : (i32) -> tensor<32x1xi32, #blocked0>
    %17 = tt.splat %arg2 : (i32) -> tensor<32x1xi32, #blocked1>
    %18 = "triton_gpu.cmpi"(%8, %16) {predicate = 2 : i64} : (tensor<32x1xi32, #blocked0>, tensor<32x1xi32, #blocked0>) -> tensor<32x1xi1, #blocked0>
    %19 = "triton_gpu.cmpi"(%9, %17) {predicate = 2 : i64} : (tensor<32x1xi32, #blocked1>, tensor<32x1xi32, #blocked1>) -> tensor<32x1xi1, #blocked1>
    %20 = arith.index_cast %arg3 : i32 to index
    %21 = tt.make_range {end = 8 : i32, start = 0 : i32} : tensor<8xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>
    %22 = tt.expand_dims %21 {axis = 0 : i32} : (tensor<8xi32, #triton_gpu.slice<{dim = 0, parent = #blocked1}>>) -> tensor<1x8xi32, #blocked1>
    %23 = tt.splat %arg3 : (i32) -> tensor<1x8xi32, #blocked1>
    %24 = tt.broadcast %19 : (tensor<32x1xi1, #blocked1>) -> tensor<32x8xi1, #blocked1>
    %25 = tt.broadcast %13 : (tensor<32x1x!tt.ptr<f32>, #blocked1>) -> tensor<32x8x!tt.ptr<f32>, #blocked1>
    %26 = scf.for %arg4 = %c0 to %20 step %c8 iter_args(%arg5 = %cst) -> (tensor<32x8xf32, #blocked1>) {
      %30 = arith.index_cast %arg4 : index to i32
      %31 = tt.splat %30 : (i32) -> tensor<1x8xi32, #blocked1>
      %32 = arith.addi %31, %22 : tensor<1x8xi32, #blocked1>
      %33 = "triton_gpu.cmpi"(%32, %23) {predicate = 2 : i64} : (tensor<1x8xi32, #blocked1>, tensor<1x8xi32, #blocked1>) -> tensor<1x8xi1, #blocked1>
      %34 = tt.broadcast %33 : (tensor<1x8xi1, #blocked1>) -> tensor<32x8xi1, #blocked1>
      %35 = arith.andi %24, %34 : tensor<32x8xi1, #blocked1>
      %36 = tt.broadcast %32 : (tensor<1x8xi32, #blocked1>) -> tensor<32x8xi32, #blocked1>
      %37 = tt.addptr %25, %36 : tensor<32x8x!tt.ptr<f32>, #blocked1>, tensor<32x8xi32, #blocked1>
      %38 = tt.load %37, %35, %cst {cache = 1 : i32, evict = 1 : i32, isVolatile = false} : tensor<32x8xf32, #blocked1>
      %39 = "triton_gpu.cmpf"(%arg5, %38) {predicate = 2 : i64} : (tensor<32x8xf32, #blocked1>, tensor<32x8xf32, #blocked1>) -> tensor<32x8xi1, #blocked1>
      %40 = "triton_gpu.select"(%39, %arg5, %38) : (tensor<32x8xi1, #blocked1>, tensor<32x8xf32, #blocked1>, tensor<32x8xf32, #blocked1>) -> tensor<32x8xf32, #blocked1>
      scf.yield %40 : tensor<32x8xf32, #blocked1>
    }
    %27 = tt.reduce %26 {axis = 1 : i32, redOp = 12 : i32} : tensor<32x8xf32, #blocked1> -> tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>
    %28 = triton_gpu.convert_layout %27 : (tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked1}>>) -> tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>
    %29 = tt.expand_dims %28 {axis = 1 : i32} : (tensor<32xf32, #triton_gpu.slice<{dim = 1, parent = #blocked0}>>) -> tensor<32x1xf32, #blocked0>
    tt.store %15, %29, %18 : tensor<32x1xf32, #blocked0>
    return
  }
}
