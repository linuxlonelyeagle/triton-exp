//
// Generated by LLVM NVPTX Back-End
//

.version 7.5
.target sm_80
.address_size 64

	// .globl	sum_kernel_1_0d1d2
.extern .shared .align 1 .b8 global_smem[];

.visible .entry sum_kernel_1_0d1d2(
	.param .u64 sum_kernel_1_0d1d2_param_0,
	.param .u64 sum_kernel_1_0d1d2_param_1,
	.param .u32 sum_kernel_1_0d1d2_param_2
)
.maxntid 128, 1, 1
{
	.reg .pred 	%p<6>;
	.reg .b32 	%r<27>;
	.reg .b64 	%rd<41>;

	ld.param.u64 	%rd8, [sum_kernel_1_0d1d2_param_0];
	ld.param.u64 	%rd9, [sum_kernel_1_0d1d2_param_1];
	mov.u32 	%r19, %tid.x;
	and.b32  	%r20, %r19, 31;
	ld.param.u32 	%r21, [sum_kernel_1_0d1d2_param_2];
	and.b32  	%r22, %r19, 15;
	mov.u32 	%r23, %ctaid.x;
	shl.b32 	%r24, %r23, 4;
	or.b32  	%r25, %r24, %r22;
	mul.wide.s32 	%rd10, %r25, 8;
	add.s64 	%rd2, %rd8, %rd10;
	setp.lt.s32 	%p1, %r25, %r21;
	mov.u64 	%rd3, 0;
	@%p1 ld.global.b64 { %rd1 }, [ %rd2 + 0 ];
	@!%p1 mov.u64 %rd1, %rd3;
	shr.u64 	%rd11, %rd1, 32;
	cvt.u32.u64 	%r4, %rd11;
	cvt.u32.u64 	%r2, %rd1;
	setp.eq.s32 	%p3, %r20, 0;
	shfl.sync.bfly.b32 %r1, %r2, 0x8, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r3, %r4, 0x8, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd12, %r1;
	cvt.u64.u32 	%rd13, %r3;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd15, %rd12, %rd14;
	add.s64 	%rd16, %rd1, %rd15;
	shr.u64 	%rd17, %rd16, 32;
	cvt.u32.u64 	%r8, %rd17;
	cvt.u32.u64 	%r6, %rd16;
	shfl.sync.bfly.b32 %r5, %r6, 0x4, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r7, %r8, 0x4, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd18, %r5;
	cvt.u64.u32 	%rd19, %r7;
	shl.b64 	%rd20, %rd19, 32;
	or.b64  	%rd21, %rd18, %rd20;
	add.s64 	%rd22, %rd16, %rd21;
	shr.u64 	%rd23, %rd22, 32;
	cvt.u32.u64 	%r12, %rd23;
	cvt.u32.u64 	%r10, %rd22;
	shfl.sync.bfly.b32 %r9, %r10, 0x2, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r11, %r12, 0x2, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd24, %r9;
	cvt.u64.u32 	%rd25, %r11;
	shl.b64 	%rd26, %rd25, 32;
	or.b64  	%rd27, %rd24, %rd26;
	add.s64 	%rd28, %rd22, %rd27;
	shr.u64 	%rd29, %rd28, 32;
	cvt.u32.u64 	%r16, %rd29;
	cvt.u32.u64 	%r14, %rd28;
	shfl.sync.bfly.b32 %r13, %r14, 0x1, 0x1f, 0xffffffff;
	shfl.sync.bfly.b32 %r15, %r16, 0x1, 0x1f, 0xffffffff;
	cvt.u64.u32 	%rd30, %r13;
	cvt.u64.u32 	%rd31, %r15;
	shl.b64 	%rd32, %rd31, 32;
	or.b64  	%rd33, %rd30, %rd32;
	add.s64 	%rd4, %rd28, %rd33;
	mov.u32 	%r17, global_smem;
	@%p3 st.shared.b64 [ %r17 + 0 ], %rd4;
	bar.sync 	0;
	shl.b32 	%r26, %r19, 3;
	add.s32 	%r18, %r17, %r26;
	ld.shared.u32 	%rd34, [%r18];
	ld.shared.u32 	%rd35, [%r18+4];
	shl.b64 	%rd36, %rd35, 32;
	or.b64  	%rd5, %rd36, %rd34;
	setp.lt.s32 	%p4, %r19, 1;
	@%p4 st.shared.b64 [ %r18 + 0 ], %rd5;
	bar.sync 	0;
	ld.shared.u32 	%rd37, [global_smem+4];
	shl.b64 	%rd38, %rd37, 32;
	ld.shared.u32 	%rd39, [global_smem];
	or.b64  	%rd6, %rd38, %rd39;
	mul.wide.s32 	%rd40, %r23, 8;
	add.s64 	%rd7, %rd9, %rd40;
	mov.pred 	%p5, -1;
	@%p5 st.global.b64 [ %rd7 + 0 ], { %rd6 };
	ret;

}
